2023-05-02 13:39:57.266 | DEBUG    | __main__:train:60 - Random seed is fixed for training.
2023-05-02 13:39:57.266 | INFO     | __main__:train:66 - Loading the data...
2023-05-02 13:39:57.271 | INFO     | __main__:train:70 - Data is loaded.
2023-05-02 13:39:57.272 | INFO     | __main__:train:73 - Starting tokenizing...
2023-05-02 13:39:57.404 | INFO     | __main__:train:76 - Tokenizing is done.
2023-05-02 13:39:57.404 | INFO     | __main__:train:79 - Saving tokenizer...
2023-05-02 13:39:57.407 | INFO     | __main__:train:81 - Tokenizer is saved.
2023-05-02 13:39:57.407 | INFO     | __main__:train:84 - Preparing data loaders...
2023-05-02 13:39:57.408 | INFO     | __main__:train:102 - Data loaders are prepared.
2023-05-02 13:39:57.408 | INFO     | __main__:train:106 - Staring training...
2023-05-02 13:39:58.211 | DEBUG    | src.model.gpt_language_model.gpt:__init__:114 - GPT language model is created with number of parameters: 38.39 million
2023-05-02 13:39:58.212 | DEBUG    | __main__:train:131 - LR warmup iters: 12543
2023-05-02 13:39:58.212 | DEBUG    | __main__:train:137 - LR decay iters: 119162
2023-05-02 13:39:58.251 | DEBUG    | src.model.trainer:train:122 - Training on 'cpu' device
2023-05-02 14:21:29.785 | DEBUG    | __main__:train:60 - Random seed is fixed for training.
2023-05-02 14:21:29.786 | INFO     | __main__:train:66 - Loading the data...
2023-05-02 14:21:29.809 | INFO     | __main__:train:70 - Data is loaded.
2023-05-02 14:21:29.809 | INFO     | __main__:train:73 - Starting tokenizing...
2023-05-02 14:21:29.943 | INFO     | __main__:train:76 - Tokenizing is done.
2023-05-02 14:21:29.943 | INFO     | __main__:train:79 - Saving tokenizer...
2023-05-02 14:21:29.946 | INFO     | __main__:train:81 - Tokenizer is saved.
2023-05-02 14:21:29.946 | INFO     | __main__:train:84 - Preparing data loaders...
2023-05-02 14:21:29.947 | INFO     | __main__:train:102 - Data loaders are prepared.
2023-05-02 14:21:29.947 | INFO     | __main__:train:106 - Staring training...
2023-05-02 14:21:30.182 | DEBUG    | src.model.gpt_language_model.gpt:__init__:114 - GPT language model is created with number of parameters: 10.65 million
2023-05-02 14:21:30.183 | DEBUG    | __main__:train:131 - LR warmup iters: 1568
2023-05-02 14:21:30.183 | DEBUG    | __main__:train:137 - LR decay iters: 14897
2023-05-02 14:21:30.288 | DEBUG    | src.model.trainer:train:122 - Training on 'cpu' device
2023-05-03 10:37:03.129 | DEBUG    | __main__:train:60 - Random seed is fixed for training.
2023-05-03 10:37:03.130 | INFO     | __main__:train:66 - Loading the data...
2023-05-03 10:37:03.171 | INFO     | __main__:train:70 - Data is loaded.
2023-05-03 10:37:03.172 | INFO     | __main__:train:73 - Starting tokenizing...
2023-05-03 10:37:03.447 | INFO     | __main__:train:76 - Tokenizing is done.
2023-05-03 10:37:03.447 | INFO     | __main__:train:79 - Saving tokenizer...
2023-05-03 10:37:03.466 | INFO     | __main__:train:81 - Tokenizer is saved.
2023-05-03 10:37:03.467 | INFO     | __main__:train:84 - Preparing data loaders...
2023-05-03 10:37:03.500 | INFO     | __main__:train:102 - Data loaders are prepared.
2023-05-03 10:37:03.501 | INFO     | __main__:train:106 - Staring training...
2023-05-03 10:37:03.980 | DEBUG    | src.model.gpt_language_model.gpt:__init__:114 - GPT language model is created with number of parameters: 10.65 million
2023-05-03 10:37:03.981 | DEBUG    | __main__:train:131 - LR warmup iters: 1568
2023-05-03 10:37:03.981 | DEBUG    | __main__:train:137 - LR decay iters: 14897
2023-05-03 10:37:04.092 | DEBUG    | src.model.trainer:train:122 - Training on 'cpu' device
2023-05-03 10:52:01.340 | DEBUG    | __main__:train:60 - Random seed is fixed for training.
2023-05-03 10:52:01.370 | INFO     | __main__:train:66 - Loading the data...
2023-05-03 10:52:01.423 | INFO     | __main__:train:70 - Data is loaded.
2023-05-03 10:52:01.423 | INFO     | __main__:train:73 - Starting tokenizing...
2023-05-03 10:52:01.591 | INFO     | __main__:train:76 - Tokenizing is done.
2023-05-03 10:52:01.592 | INFO     | __main__:train:79 - Saving tokenizer...
2023-05-03 10:52:01.615 | INFO     | __main__:train:81 - Tokenizer is saved.
2023-05-03 10:52:01.616 | INFO     | __main__:train:84 - Preparing data loaders...
2023-05-03 10:52:01.616 | INFO     | __main__:train:102 - Data loaders are prepared.
2023-05-03 10:52:01.617 | INFO     | __main__:train:106 - Staring training...
2023-05-03 10:52:01.620 | DEBUG    | __main__:train:131 - LR warmup iters: 0
2023-05-03 10:52:01.620 | DEBUG    | __main__:train:137 - LR decay iters: 31371
2023-05-03 10:52:01.714 | DEBUG    | src.model.trainer:train:122 - Training on 'cpu' device
2023-05-03 10:53:03.790 | INFO     | src.model.trainer:train:151 - Current eval loss is `2.5011` which is smaller than current best loss of `inf`; saving the model...
2023-05-03 10:53:03.824 | INFO     | src.model.trainer:train:157 - Best model is saved.
2023-05-03 10:53:03.824 | INFO     | __main__:train:156 - Training is finished
