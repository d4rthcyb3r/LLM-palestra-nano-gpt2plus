seed: 42

data:
  path:
    raw: data/raw
    interim: data/interim

datasets:
  tiny_shakespeare:
    url: https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt
    folder: ${data.path.raw}/tiny_shakespeare
    filename: input.txt
    file_path: ${datasets.tiny_shakespeare.folder}/${datasets.tiny_shakespeare.filename}

dataloader:
  test_split: 0.9
  num_workers: 4

model:
  checkpoint_folder: "models"
  tokenizer_folder: ${model.checkpoint_folder}/tokenizers
  bigram:
    size:
      large:
        context_size: 8 # what is the maximum number of tokens to use during forward pass
        batch_size: 32
        # optimizer
        learning_rate: 1e-2
        weight_decay: 1e-1
        betas:
          - 0.9
          - 0.99
        # lr schedular
        warmup_iters: 0.0
        lr_decay_iters: 1.0
        grad_accumulation_steps: 1
        epochs: 1
        tqdm_update_interval: 100
        checkpoint_model_path: ${model.checkpoint_folder}/bigram_model.pth.tar
        tokenizer_path: ${model.tokenizer_folder}/tokenizer_bigram.pkl
  gpt:
    size:
      large:
        embeddings_size: 516
        context_size: 384
        head_size: null # either should be provided explicitly or calculated implicitly as embeddings_size // num_heads
        num_heads: 12
        feed_forward_scaling: 4
        num_layers: 12
        bias: false # bias in Linear layers and LayerNorms: if true - like in GPT-2, false - a bit better and faster
        dropout: 0.2
        weight_tying: true
        batch_size: 8
        # optimizer
        learning_rate: 3e-4
        weight_decay: 1e-1
        betas:
          - 0.9
          - 0.99
        # lr schedular
        warmup_iters: null
        lr_decay_iters: null
        grad_accumulation_steps: 5
        epochs: 1
        tqdm_update_interval: 1
        checkpoint_model_path: ${model.checkpoint_folder}/gpt_model_large.pth.tar
        tokenizer_path: ${model.tokenizer_folder}/tokenizer_gpt_large.pkl
      medium:
        embeddings_size: 384
        context_size: 256
        head_size: null
        num_heads: 6
        feed_forward_scaling: 4
        num_layers: 6
        bias: false
        dropout: 0.2
        weight_tying: true
        batch_size: 64
        # optimizer
        learning_rate: 1e-3
        weight_decay: 1e-1
        betas:
          - 0.9
          - 0.99
        # lr schedular
        warmup_iters: null
        lr_decay_iters: null
        grad_accumulation_steps: null
        epochs: 1
        tqdm_update_interval: 1
        checkpoint_model_path: ${model.checkpoint_folder}/gpt_model_medium.pth.tar
        tokenizer_path: ${model.tokenizer_folder}/tokenizer_gpt_medium.pkl
      small:
        embeddings_size: 128
        context_size: 64
        head_size: null
        num_heads: 4
        feed_forward_scaling: 4
        num_layers: 4
        bias: false
        dropout: 0.2
        weight_tying: true
        batch_size: 64
        # optimizer
        learning_rate: 3e-4
        weight_decay: 1e-1
        betas:
          - 0.9
          - 0.95
        # lr schedular
        warmup_iters: null
        lr_decay_iters: null
        grad_accumulation_steps: null
        epochs: 1
        tqdm_update_interval: 10
        checkpoint_model_path: ${model.checkpoint_folder}/gpt_model_small.pth.tar
        tokenizer_path: ${model.tokenizer_folder}/tokenizer_gpt_small.pkl

logs:
  folder: logs
  logger_kwargs:
    rotation: 50 MB
  training: ${logs.folder}/training.log
  generation: ${logs.folder}/generation.log
