{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjGcvdA1tgzm"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Andrei-Aksionov/nanoGPTplus/blob/main/notebooks/examples/run_on_google_colab.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPzljRMpsKgr"
      },
      "source": [
        "<h1><center>RUNNING TRAINING AND SAMPLING</center></h1>\n",
        "<h5><center>in Google Colaboratory</center></h5>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RsFy6wqsq05"
      },
      "source": [
        "Se você não possui uma GPU ou não deseja instalar este projeto em sua máquina local, pode executar este notebook no Google Colab. Este serviço fornece uma instância com CPU, GPU e TPU (esta última não usaremos).\n",
        "\n",
        "Para fazer isso você pode:\n",
        "1. Clique no emblema `Abrir no Colab`.\n",
        "2. Copie este notebook para o [Google Colab](https://colab.research.google.com/) manualmente e execute-o lá."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2fMRCTftl_X"
      },
      "source": [
        "# 1. Preparations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZm3BT5Dtpy2"
      },
      "source": [
        "Primeiro precisamos verificar se a instância está pronta, depois clonar o repositório, criar o ambiente virtual e instalar todas as dependências com o próprio projeto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZvKha4SuBB0"
      },
      "source": [
        "## 1.1. Runtime type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNmuWm3UAzUX"
      },
      "source": [
        "O Google Colab fornece instâncias de CPU, GPU e TPU.\n",
        "\n",
        "O código funcionará na CPU e na GPU, mas recomendo usar a instância da GPU apenas por uma questão de velocidade.\n",
        "\n",
        "Aqui está um [link](https://www.tutorialspoint.com/google_colab/google_colab_using_free_gpu.htm) sobre como alterar o tipo de tempo de execução."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJpY3Jemuqyf"
      },
      "source": [
        "Se a GPU estiver selecionada e disponível, o código abaixo exibirá informações sobre a GPU disponível e seu status atual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOMt2BC0AWC5",
        "outputId": "3cd237be-f96d-47f0-c199-e1d900e74534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jun  1 16:08:51 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoA5TRPWvAzG"
      },
      "source": [
        "## 1.2. Clone repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-kqQF--QD5Y",
        "outputId": "dd2e4aec-0625-46f3-efa1-d7c042ebc301"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLM-palestra-nano-gpt2plus'...\n",
            "remote: Enumerating objects: 103, done.\u001b[K\n",
            "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
            "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
            "remote: Total 103 (delta 4), reused 103 (delta 4), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (103/103), 997.00 KiB | 28.49 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/d4rthcyb3r/LLM-palestra-nano-gpt2plus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLcK09smvX3a"
      },
      "source": [
        "Colab permite `cd` em um diretório. Isso significa que a partir de agora todos os comandos serão executados a partir deste diretório."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTTDQDUe1buZ",
        "outputId": "39416e39-f078-4ed1-be4a-09c7ed4740c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLM-palestra-nano-gpt2plus\n",
            "data\t logs\t notebooks    pyproject.toml  references  tests\n",
            "LICENSE  models  poetry.lock  README.md       src\n"
          ]
        }
      ],
      "source": [
        "%cd LLM-palestra-nano-gpt2plus/\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGDbvBREvmS1"
      },
      "source": [
        "## 1.3. Prepare virtual environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipBnvU05vq-E"
      },
      "source": [
        "Cada instância do Google Colab vem com uma infinidade de pacotes pré-instalados. Mas para reprodutibilidade no futuro, como não controlo as versões de todos os pacotes pré-instalados, prefiro criar um novo ambiente virtual vazio e instalar as dependências do projeto nele."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdmB78obwL9Q"
      },
      "source": [
        "**Nota Importante**: Pode se alterar pacotes, mas definitivamente\n",
        "\n",
        "---\n",
        "\n",
        "não pode controlar a versão do interpretador python. Por enquanto é 3,9. Se você tiver algum problema com a execução das células, primeiro verifique se a saída da célula abaixo é `Python 3.9.*`.\n",
        "\n",
        "O projeto deveria funcionar em python 3.8 e superior, mas 3.8 não foi testado no Colab, apenas 3.9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvaEJYEewluP",
        "outputId": "6a61c591-9e79-4290-9e51-5dc2a3201600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.11\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-_adTWI188f",
        "outputId": "5419f8b8-1f01-4523-bc95-cd6a912cfb23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hcreated virtual environment CPython3.10.11.final.0-64 in 793ms\n",
            "  creator CPython3Posix(dest=/content/LLM-palestra-nano-gpt2plus/venv, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: pip==23.1.2, setuptools==67.7.2, wheel==0.40.0\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"
          ]
        }
      ],
      "source": [
        "# instale o pacote que permite criar ambientes virtuais e criar `venv` dentro da pasta do projeto\n",
        "%pip install --quiet virtualenv\n",
        "!virtualenv venv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKy5fH6gxK8h"
      },
      "source": [
        "*Um* truque para ativar o ambiente virtual permanentemente, para que todos os comandos sejam executados dentro deste ambiente, é alterar a variável de ambiente `$PATH`, para que o venv seja o primeiro da linha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7_cBPZwLHag",
        "outputId": "f832b845-fa20-45d3-8dd3-e9e43194cc9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLM-palestra-nano-gpt2plus/venv/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# existem algumas dificuldades com a exportação padrão da variável de ambiente,\n",
        "# então eu uso o módulo `os` interno do python\n",
        "os.environ[\"PATH\"] = f\"{os.getcwd()}/venv/bin:{os.environ['PATH']}\"\n",
        "!echo $PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-WYvliox0sr"
      },
      "source": [
        "*Agora* podemos verificar se o venv está ativado: o comando abaixo mostra que realmente é um ambiente virtual vazio.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rINXOd4lAMLo",
        "outputId": "6394d1df-4ca7-46b1-cd55-f529e328294c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package    Version\n",
            "---------- -------\n",
            "pip        23.1.2\n",
            "setuptools 67.7.2\n",
            "wheel      0.40.0\n"
          ]
        }
      ],
      "source": [
        "!pip list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-0nOE62x__m"
      },
      "source": [
        "Agora podemos instalar as dependências especificadas em `pyproject.toml` em nosso venv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EssNhOKdBgLS",
        "outputId": "f214f28b-f15d-4040-bc01-8ab4714aaafb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.7/797.7 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.1/282.1 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.9/426.9 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.4/117.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.0/157.0 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m123.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building editable for nanogptplus (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install --quiet -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_De9EFEly1QO"
      },
      "source": [
        "# 2. Running models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoKKb7lXy4BU"
      },
      "source": [
        "Agora estamos todos prontos.\n",
        "\n",
        "Podemos treinar o modelo Bigram, extrair dele novos tokens. Faça o mesmo para o GPT e até carregue o peso do modelo GPT2 pré-treinado do Huggingface e use-o para uma nova amostragem de token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGTlj-CwzL1T"
      },
      "source": [
        "## 2.1. Download dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuBMMZ_jzR-B"
      },
      "source": [
        "Para simplificar, este projeto usa um pequeno conjunto de dados de Shakespeare. Você pode definitivamente usar o seu próprio. Você pode verificar o README sobre o que precisa ser feito."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgFz_bvGCzrs",
        "outputId": "e58e4c2a-635f-456b-f00d-d8143228b7fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2023-06-01 16:25:29.396\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.data.downloader\u001b[0m:\u001b[36mdownload\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mDownloading https://raw.githubusercontent.com/d4rthcyb3r/LLM-palestra-nano-gpt2plus/main/data/raw/tiny_shakespeare/input.txt into /content/LLM-palestra-nano-gpt2plus/data/raw/tiny_shakespeare\u001b[0m\n",
            "\u001b[32m2023-06-01 16:25:29.848\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.data.downloader\u001b[0m:\u001b[36mdownload\u001b[0m:\u001b[36m44\u001b[0m - \u001b[34m\u001b[1mDownloading is finished\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python src/data/scripts/download_tiny_shakespeare.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BnECvLPzos-"
      },
      "source": [
        "## 2.2. Bigram language model - TEST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRYixAgrztMv"
      },
      "source": [
        "Primeiro, começamos com algo bastante simples: modelo de linguagem bigrama. Esse modelo apenas aprende qual token é o mais frequente após o atual e usa essas estatísticas durante a amostragem. Mais sobre isso em `src/model/bigram_language_model/README.md`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqPjJzzCC9VZ",
        "outputId": "ad5de1fc-6839-4130-8b42-3dfa459985ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-03-18 14:16:44.527\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m60\u001b[0m - \u001b[34m\u001b[1mRandom seed is fixed for training.\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mLoading the data...\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mData is loaded.\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mStarting tokenizing...\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.662\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mTokenizing is done.\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1mSaving tokenizer...\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m81\u001b[0m - \u001b[1mTokenizer is saved.\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.666\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mPreparing data loaders...\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.666\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mData loaders are prepared.\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mStaring training...\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.669\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m131\u001b[0m - \u001b[34m\u001b[1mLR warmup iters: 0\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.670\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m137\u001b[0m - \u001b[34m\u001b[1mLR decay iters: 31371\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:46.915\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m122\u001b[0m - \u001b[34m\u001b[1mTraining on 'cuda' device\u001b[0m\n",
            "=============== Epoch: 0 ===============\n",
            "train: 100% 31371/31371 [01:34<00:00, 332.72it/s, loss=2.47]\n",
            "eval: 100% 3486/3486 [00:08<00:00, 406.43it/s, loss=2.5]\n",
            "Eval averaged loss: 2.5011\n",
            "\u001b[32m2023-03-18 14:18:29.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mCurrent eval loss is `2.5011` which is smaller than current best loss of `inf`; saving the model...\u001b[0m\n",
            "\u001b[32m2023-03-18 14:18:29.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mBest model is saved.\u001b[0m\n",
            "\u001b[32m2023-03-18 14:18:29.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m156\u001b[0m - \u001b[1mTraining is finished\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# para bigrama, apenas o tamanho grande está disponível\n",
        "!python src/model/train.py bigram --size large"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU1clRmy0qx7"
      },
      "source": [
        "E agora podemos amostrar do modelo treinado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCYLOa8VDmeR",
        "outputId": "967020de-d31a-4f1f-b9ad-95abf465f282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2023-06-01 16:26:26.246\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mRandom seed is fixed for token generation.\u001b[0m\n",
            "\u001b[32m2023-06-01 16:26:27.870\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m104\u001b[0m - \u001b[34m\u001b[1mGenerating tokens on 'cuda' device\u001b[0m\n",
            "\u001b[32m2023-06-01 16:26:27.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mNew generated tokens:  d\n",
            "O: as; nte tis, te othut mod thand he, preckn,\n",
            "\n",
            "Henthif o--wishelapinisers we s, orean,\n",
            "TAUCluprt,\u001b[0m\n",
            "\u001b[32m2023-06-01 16:26:27.900\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m121\u001b[0m - \u001b[34m\u001b[1mToken generation took: 0.0305 seconds\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python src/model/generate.py bigram --size large --max-new-tokens 100 --fix-seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOIzaYme1DJk"
      },
      "source": [
        "Sim, o modelo é rápido.\n",
        "A saída é um pouco semelhante a palavras reais, o que é bom para um modelo tão simples.\n",
        "\n",
        "Mas ainda assim, não é algo entendivel.\n",
        "\n",
        "Vamos verificar o que o GPT pode oferecer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIIMZ9Jo1j59"
      },
      "source": [
        "## 2.3. GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrVi1EFu1tjq"
      },
      "source": [
        "**GPT** aceita três tamanhos: `small`, `medium` e `large`.\n",
        "\n",
        "Pequeno é bom para depuração, enquanto para resultados mais ou menos bons, é claro, quanto maior o modelo, melhor. Além disso, você pode jogar com o argumento `--dataset-fraction`, que especifica qual porção/fração do conjunto de dados usar para treinamento.\n",
        "\n",
        "Como o tokenizador é bastante simples e o treinamento pode demorar um pouco, vamos usar apenas 10% do conjunto de dados. Embora você possa tentar usar o conjunto de dados completo se tiver uma GPU mais poderosa em sua posse (por exemplo, no Google Colab Pro/Pro+)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AQZ6HhaH7XAf"
      },
      "outputs": [],
      "source": [
        "os.environ[\"gpt_size\"] = \"medium\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT4Q1reoEW8r",
        "outputId": "7ba17e3b-6c60-4db8-b720-51c6d8b8764f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2023-06-01 16:27:00.025\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m60\u001b[0m - \u001b[34m\u001b[1mRandom seed is fixed for training.\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.026\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mLoading the data...\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mData is loaded.\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mStarting tokenizing...\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mTokenizing is done.\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1mSaving tokenizer...\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m81\u001b[0m - \u001b[1mTokenizer is saved.\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mPreparing data loaders...\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.187\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mData loaders are prepared.\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.187\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mStaring training...\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.388\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m114\u001b[0m - \u001b[34m\u001b[1mGPT language model is created with number of parameters: 10.65 million\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.389\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m131\u001b[0m - \u001b[34m\u001b[1mLR warmup iters: 156\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.389\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m137\u001b[0m - \u001b[34m\u001b[1mLR decay iters: 1490\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:01.973\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m122\u001b[0m - \u001b[34m\u001b[1mTraining on 'cuda' device\u001b[0m\n",
            "=============== Epoch: 0 ===============\n",
            "train: 100% 1569/1569 [12:12<00:00,  2.14it/s, loss=1.23]\n",
            "eval: 100% 174/174 [00:28<00:00,  6.19it/s, loss=3.4]\n",
            "Eval averaged loss: 3.4022\n",
            "\u001b[32m2023-06-01 16:39:42.729\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mCurrent eval loss is `3.4022` which is smaller than current best loss of `inf`; saving the model...\u001b[0m\n",
            "\u001b[32m2023-06-01 16:39:42.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mBest model is saved.\u001b[0m\n",
            "\u001b[32m2023-06-01 16:39:42.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m156\u001b[0m - \u001b[1mTraining is finished\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python src/model/train.py gpt --size $gpt_size --dataset-fraction 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzd0xSId2Hnp"
      },
      "source": [
        "*E* os novos tokens são:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGW67vbyMamT",
        "outputId": "f061904b-3d78-401c-d697-05c9da9ccdd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2023-06-01 16:39:45.112\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mRandom seed is fixed for token generation.\u001b[0m\n",
            "\u001b[32m2023-06-01 16:39:45.482\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m114\u001b[0m - \u001b[34m\u001b[1mGPT language model is created with number of parameters: 10.65 million\u001b[0m\n",
            "\u001b[32m2023-06-01 16:39:47.630\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m104\u001b[0m - \u001b[34m\u001b[1mGenerating tokens on 'cuda' device\u001b[0m\n",
            "100% 1000/1000 [00:08<00:00, 114.52it/s]\n",
            "\u001b[32m2023-06-01 16:39:56.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mNew generated tokens:  d\n",
            "Our sufferance. There's ne'er arm in the war,\n",
            "Our still on in sufferity, or be some of them our truth: and deliver him\n",
            "Which our distinction; and it our shall answer\n",
            "The treasure of our strange.\n",
            "\n",
            "MENENIUS:\n",
            "Now, be gone, beseech you.\n",
            "\n",
            "CORIOLANUS:\n",
            "That I am sworn! this content:\n",
            "To bring unsul to be so, I do beseech you,\n",
            "Let me deserve consul, and sake me be consul.\n",
            "\n",
            "CORIOLANUS:\n",
            "Is thou own did fairst this should deeds,\n",
            "When I am so honour'd rather was to them; I think\n",
            "And such a soldier: if the lies if the world\n",
            "be from our Cominius.\n",
            "\n",
            "MENENIUS:\n",
            "And, let's well; sir,'tis a below heard.\n",
            "\n",
            "COMINIUS:\n",
            "Hear me speak:\n",
            "I have been so, and from their state,\n",
            "By Jove and charges me from hencests, whence\n",
            "In he did service of the nature force; mark I\n",
            "speak in the place of his country: seek meal me speak me, and the blood\n",
            "To know throw their hurs of Marcius sound not done\n",
            "How like a charge to both rest, which with his provater\n",
            "In proceed him should not be bollood, wherein he was\n",
            "And lack'd upon my co\u001b[0m\n",
            "\u001b[32m2023-06-01 16:39:56.366\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m121\u001b[0m - \u001b[34m\u001b[1mToken generation took: 8.7363 seconds\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python src/model/generate.py gpt --size $gpt_size --max-new-tokens 1000 --fix-seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UARmUd1i2OM_"
      },
      "source": [
        "Ok, isso parece muito melhor do que o que o Bigram LM fez. Não se esqueça de que o conjunto de dados é bem pequeno e o tokenizer é básico. \n",
        "Portanto, o poder do GPT não é totalmente utilizado.\n",
        "\n",
        "Além disso, você pode obter melhores resultados com modelo maior e treinamento no conjunto de dados completo, mas levará um tempo no Nvidia T4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl-igDB8_EOP"
      },
      "source": [
        "## 2.4. GPT with pretrained weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX3cFhUl2oeA"
      },
      "source": [
        "Esta implementação GPT oferece suporte ao carregamento de pesos pré-treinados para o modelo GPT2 do Huggingface (os pesos são fornecidos pela OpenAI). Esse modelo foi treinado em um grande corpo de dados e usa um [tokenizador de pares de bytes] muito mais sofisticado (https://huggingface.co/course/chapter6/5?fw=pt).\n",
        "\n",
        "**Observação**: os pesos não são pré-treinados no conjunto de dados de Shakespeare, então a saída será diferente do que vimos antes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKEbWvr73rdl"
      },
      "source": [
        "**GPT2** tem 4 configurações:\n",
        "1. gpt2 (124M parameters) \n",
        "2. gpt2-medium (350M)\n",
        "3. gpt2-large (774M)\n",
        "4. gpt2-xl (1.5B)\n",
        "\n",
        "*O Google Colab com Nvidia T4 suporta até gpt2-large.\n",
        "Embora seja possível usar até mesmo o maior, será necessário alterar a forma como o modelo é carregado.*\n",
        "\n",
        "Quanto maior o modelo, melhor a amostragem, mas significa que o consumo de memória também será aumentado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "X2u06Mat_eUC"
      },
      "outputs": [],
      "source": [
        "os.environ[\"gpt2_config\"] = \"gpt2-medium\"\n",
        "os.environ[\"max_new_tokens\"] = \"1000\"\n",
        "os.environ[\"continue_tokens\"] = \"My name is Henrique Cyber but everybody calls me \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biUnerKq_Bb2",
        "outputId": "eba01bc7-25b2-4f84-9c42-47509b9bc72a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2023-06-01 16:39:57.977\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mRandom seed is fixed for token generation.\u001b[0m\n",
            "Downloading (…)lve/main/config.json: 100% 718/718 [00:00<00:00, 4.10MB/s]\n",
            "\u001b[32m2023-06-01 16:40:03.977\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m340\u001b[0m - \u001b[34m\u001b[1mCreating GPT model with parameters: {'vocab_size': 50257, 'embeddings_size': 1024, 'context_size': 1024, 'num_layers': 24, 'num_heads': 16, 'head_size': None, 'feed_forward_scaling': 4, 'bias': True, 'dropout': 0.1}\u001b[0m\n",
            "\u001b[32m2023-06-01 16:40:13.930\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m114\u001b[0m - \u001b[34m\u001b[1mGPT language model is created with number of parameters: 353.77 million\u001b[0m\n",
            "\u001b[32m2023-06-01 16:40:13.933\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m348\u001b[0m - \u001b[34m\u001b[1mLoading pretrained Huggingface model of size 'gpt2-medium' ...\u001b[0m\n",
            "Downloading pytorch_model.bin: 100% 1.52G/1.52G [00:06<00:00, 248MB/s]\n",
            "Downloading (…)neration_config.json: 100% 124/124 [00:00<00:00, 851kB/s]\n",
            "\u001b[32m2023-06-01 16:40:25.724\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m350\u001b[0m - \u001b[34m\u001b[1mHuggingface model is loaded.\u001b[0m\n",
            "\u001b[32m2023-06-01 16:40:25.727\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m387\u001b[0m - \u001b[34m\u001b[1mStarting copying weights from pretrained Huggingface model into our implementation ...\u001b[0m\n",
            "\u001b[32m2023-06-01 16:40:26.661\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m401\u001b[0m - \u001b[34m\u001b[1mWeights are copied.\u001b[0m\n",
            "\u001b[32m2023-06-01 16:40:28.972\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m104\u001b[0m - \u001b[34m\u001b[1mGenerating tokens on 'cuda' device\u001b[0m\n",
            "100% 1000/1000 [02:31<00:00,  6.60it/s]\n",
            "\u001b[32m2023-06-01 16:43:00.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mNew generated tokens: My name is Henrique Cyber but everybody calls me ime.\"\n",
            "\n",
            "\"Cruel, isn't it?\" Angela asked, brushing off her cheek massage with the end of the her straw, continuing to eat her mere scrupulously balanced spoonful.\n",
            "\n",
            "\"It's not,\" Mr. Martin replied, his mouth full of fish fish and sautéed marinated wild potatoes Kim had lost himself in at lunch, suddenly feeling a bit more comfortable in front of his own television beaming through a set of modern-day panoramas. \"All students know, sealing electrons won't work. All students know, however.\" His face became somberness and he bit his lip anxiously, shrugging. \"It's shut down for the beating, Chief.\"\n",
            "\n",
            "Kim could wonder, if Angela's spoonful of potatoes had been larger, what Mansion would have done if she, poking around guiding a glance around campus, discovered there were... sympathetic finches. After all, if they could pick one, who would maybe find someone and introduce it to resident Megan right here in Kinsey?\n",
            "\n",
            "Maybe it was more accurate to say Park, Christine or Alina would have determined the proper spearic, parasitic line and been the ones working R-medicine programs including prompt risk assessment and adequate solution prospection/assessment, while Professor Lilly, probably the most talented and honorable take on her line now at Posada School direct to know Med; twice disassociated of its history.\n",
            "\n",
            "If all this had happened eleven years ago, with the decent pay, and for reliance of last year maybe even short touch intensive classes besides social arts...\n",
            "\n",
            "Wake up everyday much 53-53 with a 2013 Rationalizzo Whitible president of UC Berkeley Ramadan. Being the school of UC Davis deference to alumni, regular inheritance from Bar-B-Q companies as part of taking community streets as a Bay Area founder and leading that semester's quota Bike Walk, Pede-Runner today would've graduated around 2009 faded away.\n",
            "\n",
            "There's an morning killed just as hard as 15th.\n",
            "\n",
            "Main image of a candle on UCCP Puffs: http://www.upvoted.com/item.php?id=24834\n",
            "\n",
            "(in classical music we styled \"whelder\" because of the fact that bastard lives when riffing Halls claimed got 12 - Sum/Fac'tpt international offspring of's:'02 GF'sAdrenosphat for a Disco Blacc but they jumped cons under the chinlock talents)\n",
            "\n",
            "Advertisements<|endoftext|>We created Paragraph \", a method we think we can use to create custom content lightly to specific models. The logic should be largely the same as (implicit and explicit)[1].\n",
            "\n",
            "Data an API should reduce to JSON via ECMAScript 3 adapter classes .??????, while the coffee project encourages the use of Function annotations for C++)\n",
            "\n",
            "What to expect: Well, the first effect we noted from Cassandra withstands face elimination over high-level to use parmanac: first $location and then $alternativelyData which are the equivalent types fields entry as requested.\n",
            "\n",
            "What I had to work out proved a problem with the data API: The concatenation encodePositions of the data is a priori given items with index numbers, while the input is dealing with ordered data (Maps). I wrote a lookup engine for Paragraph which captured UTF-8 , but it was outside the scope of these bullet. That is, bologols are ugly raw eyes compared to parsers. But due to questionable decisions with the friends and Verisign, those tests already failed in my large skedules things, and omega one bought half warranties that a characters input is data.\n",
            "\n",
            "How have I got away from the MongoDB site meant this city is in Greece?\n",
            "\n",
            "Can Paragraph escape the template range of accessibility metacatchetics ?\n",
            "\n",
            "Does it work Jett\n",
            "\n",
            "Result is nothing enabled OverdauntingComponents same code that concludes in an overfilled Post '94 I should write PayMore.Paragraphs.\n",
            "\n",
            "Welcome to knowledge forest. I am inside the employee garden of derrio.\n",
            "\n",
            "\"Blimey, cymo obligatory fuckar everybody is agreeing with major trader\", asks alternatively data point, reco Downingandassholaday. \"FOO Yrd adequate effing late auth solution to be to yes pressure mainly to enable MasterCard as bank debit card\", fixes Toomey. \"David Knight, creator s business donatwion, has offered 5000£ for creation of 75% payety income pay out without download\", europe usages the DiscountFun\".\n",
            "\n",
            "Increase NEOM by half\n",
            "\n",
            "\n",
            "The only complaint I got: Can it be dope night just as a free Jobs?\n",
            "\n",
            "Frama must blatantly expand iPhone in anticipation of looting\n",
            "\n",
            "Compulsory layers restricted+ Split Oracle and RMI; decision\u001b[0m\n",
            "\u001b[32m2023-06-01 16:43:00.589\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m121\u001b[0m - \u001b[34m\u001b[1mToken generation took: 151.6167 seconds\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python src/model/generate.py gpt --gpt2-config $gpt2_config --max-new-tokens \"$((max_new_tokens))\" --fix-seed --continue-tokens \"$continue_tokens\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBXMcQPv4aEM"
      },
      "source": [
        "### 2.4.1. Key-Value cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8jyMTAi4eaV"
      },
      "source": [
        "Para GPT é possível usar kv-cache para acelerar a geração de novos tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNmvA2sS-PaP",
        "outputId": "f63c9eea-535d-46b3-ee3c-aa75aacae956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2023-06-01 16:44:13.391\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mRandom seed is fixed for token generation.\u001b[0m\n",
            "\u001b[32m2023-06-01 16:44:14.644\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m340\u001b[0m - \u001b[34m\u001b[1mCreating GPT model with parameters: {'vocab_size': 50257, 'embeddings_size': 1024, 'context_size': 1024, 'num_layers': 24, 'num_heads': 16, 'head_size': None, 'feed_forward_scaling': 4, 'bias': True, 'dropout': 0.1}\u001b[0m\n",
            "\u001b[32m2023-06-01 16:44:24.962\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m114\u001b[0m - \u001b[34m\u001b[1mGPT language model is created with number of parameters: 353.77 million\u001b[0m\n",
            "\u001b[32m2023-06-01 16:44:24.964\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m348\u001b[0m - \u001b[34m\u001b[1mLoading pretrained Huggingface model of size 'gpt2-medium' ...\u001b[0m\n",
            "\u001b[32m2023-06-01 16:44:30.068\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m350\u001b[0m - \u001b[34m\u001b[1mHuggingface model is loaded.\u001b[0m\n",
            "\u001b[32m2023-06-01 16:44:30.070\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m387\u001b[0m - \u001b[34m\u001b[1mStarting copying weights from pretrained Huggingface model into our implementation ...\u001b[0m\n",
            "\u001b[32m2023-06-01 16:44:31.008\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m401\u001b[0m - \u001b[34m\u001b[1mWeights are copied.\u001b[0m\n",
            "\u001b[32m2023-06-01 16:44:33.016\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m104\u001b[0m - \u001b[34m\u001b[1mGenerating tokens on 'cuda' device\u001b[0m\n",
            "100% 1000/1000 [00:20<00:00, 48.60it/s]\n",
            "\u001b[32m2023-06-01 16:44:53.595\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mNew generated tokens: My name is Henrique Cyber but everybody calls me ime.\"\n",
            "\n",
            "\"Cruel, isn't it?\" Angela asked, brushing off her cheek massage with the end of the her straw, continuing to eat her mere scrupulously balanced spoonful.\n",
            "\n",
            "\"It's not,\" Mr. Martin replied, his mouth full of fish fish and sautéed marinated wild potatoes Kim had lost himself in at lunch, suddenly feeling a bit more comfortable in front of his own television beaming through a set of modern-day panoramas. \"All students know, sealing electrons won't work. All students know, however.\" His face became somberness and he bit his lip anxiously, shrugging. \"It's shut down for the beating, Chief.\"\n",
            "\n",
            "Kim could wonder, if Angela's spoonful of potatoes had been larger, what Mansion would have done if she, poking around guiding a glance around campus, discovered there were... sympathetic finches. After all, if they could pick one, who would maybe find someone and introduce it to resident Megan right here in Kinsey?\n",
            "\n",
            "Maybe it was more accurate to say Park, Christine or Alina would have determined the proper spearic, parasitic line and been the ones working R-medicine programs including prompt risk assessment and adequate solution prospection/assessment, while Professor Lilly, probably the most talented and honorable take on her line now at Posada School direct to know Med; twice disassociated of its history.\n",
            "\n",
            "If all this had happened eleven years ago, with the decent pay, and for reliance of last year maybe even short touch intensive classes besides social arts...\n",
            "\n",
            "Wake up everyday much 53-53 with a 2013 Rationalizzo Whitible president of UC Berkeley Ramadan. Being the school of UC Davis deference to alumni, regular inheritance from Bar-B-Q companies as part of taking community streets as a Bay Area founder and leading that semester's quota Bike Walk, Pede-Runner today would've graduated around 2009 faded away.\n",
            "\n",
            "There's an morning killed just as hard as 15th.\n",
            "\n",
            "Main image of a candle on UCCP Puffs: http://www.upvoted.com/item.php?id=24834\n",
            "\n",
            "(in classical music we styled \"whelder\" because of the fact that bastard lives when riffing Halls claimed got 12 - Sum/Fac'tpt international offspring of's:'02 GF'sAdrenosphat for a Disco Blacc but they jumped cons under the chinlock talents)\n",
            "\n",
            "Advertisements<|endoftext|>We created Paragraph \", a method we think we can use to create custom content lightly to specific models. The logic should be largely the same as (implicit and explicit)[1].\n",
            "\n",
            "Data an API should reduce to JSON via ECMAScript 3 adapter classes .??????, while the coffee project encourages the use of Function annotations for C++)\n",
            "\n",
            "What to expect: Well, the first effect we noted from Cassandra withstands face elimination over high-level to use parmanac: first $location and then $alternativelyData which are the equivalent types fields entry as requested.\n",
            "\n",
            "What I had to work out proved a problem with the data API: The concatenation encodePositions of the data is a priori given items with index numbers, while the input is dealing with ordered data (Maps). I wrote a lookup engine for Paragraph which captured UTF-8 , but it was outside the scope of these bullet. That is, bologols are ugly raw eyes compared to parsers. But due to questionable decisions with the friends and Verisign, those tests already failed in my large skedules things, and omega one bought half warranties that a characters input is data.\n",
            "\n",
            "How have I got away from the MongoDB site meant this city is in Greece?\n",
            "\n",
            "Can Paragraph escape the template range of accessibility metacatchetics ?\n",
            "\n",
            "Does it work Jett\n",
            "\n",
            "Result is nothing enabled OverdauntingComponents same code that concludes in an overfilled Post '94 I should write PayMore.Paragraphs.\n",
            "\n",
            "Welcome to knowledge forest. I am inside the employee garden of derrio.\n",
            "\n",
            "\"Blimey, cymo obligatory fuckar everybody is agreeing with major trader\", asks alternatively data point, reco Downingandassholaday. \"FOO Yrd adequate effing late auth solution to be to yes pressure mainly to enable MasterCard as bank debit card\", fixes Toomey. \"David Knight, creator s business donatwion, has offered 5000£ for creation of 75% payety income pay out without download\", europe usages the DiscountFun\".\n",
            "\n",
            "Increase NEOM by half\n",
            "\n",
            "\n",
            "The only complaint I got: Can it be dope night just as a free Jobs?\n",
            "\n",
            "Frama must blatantly expand iPhone in anticipation of looting\n",
            "\n",
            "Compulsory layers restricted+ Split Oracle and RMI; decision\u001b[0m\n",
            "\u001b[32m2023-06-01 16:44:53.595\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m121\u001b[0m - \u001b[34m\u001b[1mToken generation took: 20.5790 seconds\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python src/model/generate.py gpt --gpt2-config $gpt2_config --max-new-tokens \"$((max_new_tokens))\" --fix-seed --continue-tokens \"$continue_tokens\" --use-kv-cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbx9voRD4ut1"
      },
      "source": [
        "Veja a diferença: 120-150 segundos sem cache, 18-19 segundos - com. É por isso que o kv-caching é amplamente adotado."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
