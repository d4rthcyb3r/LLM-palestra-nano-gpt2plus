{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjGcvdA1tgzm"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Andrei-Aksionov/nanoGPTplus/blob/main/notebooks/examples/run_on_google_colab.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPzljRMpsKgr"
      },
      "source": [
        "<h1><center>RUNNING TRAINING AND SAMPLING</center></h1>\n",
        "<h5><center>in Google Colaboratory</center></h5>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RsFy6wqsq05"
      },
      "source": [
        "Se você não possui uma GPU ou não deseja instalar este projeto em sua máquina local, pode executar este notebook no Google Colab. Este serviço fornece uma instância com CPU, GPU e TPU (esta última não usaremos).\n",
        "\n",
        "Para fazer isso você pode:\n",
        "1. Clique no emblema `Abrir no Colab`.\n",
        "2. Copie este notebook para o [Google Colab](https://colab.research.google.com/) manualmente e execute-o lá."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2fMRCTftl_X"
      },
      "source": [
        "# 1. Preparations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZm3BT5Dtpy2"
      },
      "source": [
        "Primeiro precisamos verificar se a instância está pronta, depois clonar o repositório, criar o ambiente virtual e instalar todas as dependências com o próprio projeto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZvKha4SuBB0"
      },
      "source": [
        "## 1.1. Runtime type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNmuWm3UAzUX"
      },
      "source": [
        "O Google Colab fornece instâncias de CPU, GPU e TPU.\n",
        "\n",
        "O código funcionará na CPU e na GPU, mas recomendo usar a instância da GPU apenas por uma questão de velocidade.\n",
        "\n",
        "Aqui está um [link](https://www.tutorialspoint.com/google_colab/google_colab_using_free_gpu.htm) sobre como alterar o tipo de tempo de execução."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJpY3Jemuqyf"
      },
      "source": [
        "Se a GPU estiver selecionada e disponível, o código abaixo exibirá informações sobre a GPU disponível e seu status atual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOMt2BC0AWC5",
        "outputId": "3cd237be-f96d-47f0-c199-e1d900e74534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jun  1 16:08:51 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoA5TRPWvAzG"
      },
      "source": [
        "## 1.2. Clone repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-kqQF--QD5Y",
        "outputId": "dd2e4aec-0625-46f3-efa1-d7c042ebc301"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLM-palestra-nano-gpt2plus'...\n",
            "remote: Enumerating objects: 103, done.\u001b[K\n",
            "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
            "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
            "remote: Total 103 (delta 4), reused 103 (delta 4), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (103/103), 997.00 KiB | 28.49 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/d4rthcyb3r/LLM-palestra-nano-gpt2plus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLcK09smvX3a"
      },
      "source": [
        "Colab permite `cd` em um diretório. Isso significa que a partir de agora todos os comandos serão executados a partir deste diretório."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTTDQDUe1buZ",
        "outputId": "39416e39-f078-4ed1-be4a-09c7ed4740c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLM-palestra-nano-gpt2plus\n",
            "data\t logs\t notebooks    pyproject.toml  references  tests\n",
            "LICENSE  models  poetry.lock  README.md       src\n"
          ]
        }
      ],
      "source": [
        "%cd LLM-palestra-nano-gpt2plus/\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGDbvBREvmS1"
      },
      "source": [
        "## 1.3. Prepare virtual environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipBnvU05vq-E"
      },
      "source": [
        "Cada instância do Google Colab vem com uma infinidade de pacotes pré-instalados. Mas para reprodutibilidade no futuro, como não controlo as versões de todos os pacotes pré-instalados, prefiro criar um novo ambiente virtual vazio e instalar as dependências do projeto nele."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdmB78obwL9Q"
      },
      "source": [
        "**Nota Importante**: Pode se alterar pacotes, mas definitivamente\n",
        "\n",
        "---\n",
        "\n",
        "não pode controlar a versão do interpretador python. Por enquanto é 3,9. Se você tiver algum problema com a execução das células, primeiro verifique se a saída da célula abaixo é `Python 3.9.*`.\n",
        "\n",
        "O projeto deveria funcionar em python 3.8 e superior, mas 3.8 não foi testado no Colab, apenas 3.9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvaEJYEewluP",
        "outputId": "6a61c591-9e79-4290-9e51-5dc2a3201600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.11\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-_adTWI188f",
        "outputId": "5419f8b8-1f01-4523-bc95-cd6a912cfb23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hcreated virtual environment CPython3.10.11.final.0-64 in 793ms\n",
            "  creator CPython3Posix(dest=/content/LLM-palestra-nano-gpt2plus/venv, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: pip==23.1.2, setuptools==67.7.2, wheel==0.40.0\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"
          ]
        }
      ],
      "source": [
        "# instale o pacote que permite criar ambientes virtuais e criar `venv` dentro da pasta do projeto\n",
        "%pip install --quiet virtualenv\n",
        "!virtualenv venv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKy5fH6gxK8h"
      },
      "source": [
        "*Um* truque para ativar o ambiente virtual permanentemente, para que todos os comandos sejam executados dentro deste ambiente, é alterar a variável de ambiente `$PATH`, para que o venv seja o primeiro da linha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7_cBPZwLHag",
        "outputId": "f832b845-fa20-45d3-8dd3-e9e43194cc9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLM-palestra-nano-gpt2plus/venv/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# existem algumas dificuldades com a exportação padrão da variável de ambiente,\n",
        "# então eu uso o módulo `os` interno do python\n",
        "os.environ[\"PATH\"] = f\"{os.getcwd()}/venv/bin:{os.environ['PATH']}\"\n",
        "!echo $PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-WYvliox0sr"
      },
      "source": [
        "*Agora* podemos verificar se o venv está ativado: o comando abaixo mostra que realmente é um ambiente virtual vazio.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rINXOd4lAMLo",
        "outputId": "6394d1df-4ca7-46b1-cd55-f529e328294c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package    Version\n",
            "---------- -------\n",
            "pip        23.1.2\n",
            "setuptools 67.7.2\n",
            "wheel      0.40.0\n"
          ]
        }
      ],
      "source": [
        "!pip list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-0nOE62x__m"
      },
      "source": [
        "Agora podemos instalar as dependências especificadas em `pyproject.toml` em nosso venv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EssNhOKdBgLS",
        "outputId": "f214f28b-f15d-4040-bc01-8ab4714aaafb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.7/797.7 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.1/282.1 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.9/426.9 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.4/117.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.0/157.0 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m123.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building editable for nanogptplus (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install --quiet -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_De9EFEly1QO"
      },
      "source": [
        "# 2. Running models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoKKb7lXy4BU"
      },
      "source": [
        "Agora estamos todos prontos.\n",
        "\n",
        "Podemos treinar o modelo Bigram, extrair dele novos tokens. Faça o mesmo para o GPT e até carregue o peso do modelo GPT2 pré-treinado do Huggingface e use-o para uma nova amostragem de token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGTlj-CwzL1T"
      },
      "source": [
        "## 2.1. Download dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuBMMZ_jzR-B"
      },
      "source": [
        "Para simplificar, este projeto usa um pequeno conjunto de dados de Shakespeare. Você pode definitivamente usar o seu próprio. Você pode verificar o README sobre o que precisa ser feito."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgFz_bvGCzrs",
        "outputId": "e58e4c2a-635f-456b-f00d-d8143228b7fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2023-06-01 16:25:29.396\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.data.downloader\u001b[0m:\u001b[36mdownload\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mDownloading https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt into /content/LLM-palestra-nano-gpt2plus/data/raw/tiny_shakespeare\u001b[0m\n",
            "\u001b[32m2023-06-01 16:25:29.848\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.data.downloader\u001b[0m:\u001b[36mdownload\u001b[0m:\u001b[36m44\u001b[0m - \u001b[34m\u001b[1mDownloading is finished\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python src/data/scripts/download_tiny_shakespeare.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BnECvLPzos-"
      },
      "source": [
        "## 2.2. Bigram language model - TEST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRYixAgrztMv"
      },
      "source": [
        "Primeiro, começamos com algo bastante simples: modelo de linguagem bigrama. Esse modelo apenas aprende qual token é o mais frequente após o atual e usa essas estatísticas durante a amostragem. Mais sobre isso em `src/model/bigram_language_model/README.md`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqPjJzzCC9VZ",
        "outputId": "ad5de1fc-6839-4130-8b42-3dfa459985ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-03-18 14:16:44.527\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m60\u001b[0m - \u001b[34m\u001b[1mRandom seed is fixed for training.\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mLoading the data...\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mData is loaded.\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mStarting tokenizing...\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.662\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mTokenizing is done.\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1mSaving tokenizer...\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m81\u001b[0m - \u001b[1mTokenizer is saved.\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.666\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mPreparing data loaders...\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.666\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mData loaders are prepared.\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mStaring training...\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.669\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m131\u001b[0m - \u001b[34m\u001b[1mLR warmup iters: 0\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:44.670\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m137\u001b[0m - \u001b[34m\u001b[1mLR decay iters: 31371\u001b[0m\n",
            "\u001b[32m2023-03-18 14:16:46.915\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m122\u001b[0m - \u001b[34m\u001b[1mTraining on 'cuda' device\u001b[0m\n",
            "=============== Epoch: 0 ===============\n",
            "train: 100% 31371/31371 [01:34<00:00, 332.72it/s, loss=2.47]\n",
            "eval: 100% 3486/3486 [00:08<00:00, 406.43it/s, loss=2.5]\n",
            "Eval averaged loss: 2.5011\n",
            "\u001b[32m2023-03-18 14:18:29.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mCurrent eval loss is `2.5011` which is smaller than current best loss of `inf`; saving the model...\u001b[0m\n",
            "\u001b[32m2023-03-18 14:18:29.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mBest model is saved.\u001b[0m\n",
            "\u001b[32m2023-03-18 14:18:29.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m156\u001b[0m - \u001b[1mTraining is finished\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# para bigrama, apenas o tamanho grande está disponível\n",
        "!python src/model/train.py bigram --size large"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU1clRmy0qx7"
      },
      "source": [
        "E agora podemos amostrar do modelo treinado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCYLOa8VDmeR",
        "outputId": "967020de-d31a-4f1f-b9ad-95abf465f282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2023-06-01 16:26:26.246\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mRandom seed is fixed for token generation.\u001b[0m\n",
            "\u001b[32m2023-06-01 16:26:27.870\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m104\u001b[0m - \u001b[34m\u001b[1mGenerating tokens on 'cuda' device\u001b[0m\n",
            "\u001b[32m2023-06-01 16:26:27.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mNew generated tokens:  d\n",
            "O: as; nte tis, te othut mod thand he, preckn,\n",
            "\n",
            "Henthif o--wishelapinisers we s, orean,\n",
            "TAUCluprt,\u001b[0m\n",
            "\u001b[32m2023-06-01 16:26:27.900\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m121\u001b[0m - \u001b[34m\u001b[1mToken generation took: 0.0305 seconds\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python src/model/generate.py bigram --size large --max-new-tokens 100 --fix-seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOIzaYme1DJk"
      },
      "source": [
        "Sim, o modelo é rápido.\n",
        "A saída é um pouco semelhante a palavras reais, o que é bom para um modelo tão simples.\n",
        "\n",
        "Mas ainda assim, não é algo entendivel.\n",
        "\n",
        "Vamos verificar o que o GPT pode oferecer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIIMZ9Jo1j59"
      },
      "source": [
        "## 2.3. GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrVi1EFu1tjq"
      },
      "source": [
        "**GPT** aceita três tamanhos: `small`, `medium` e `large`.\n",
        "\n",
        "Pequeno é bom para depuração, enquanto para resultados mais ou menos bons, é claro, quanto maior o modelo, melhor. Além disso, você pode jogar com o argumento `--dataset-fraction`, que especifica qual porção/fração do conjunto de dados usar para treinamento.\n",
        "\n",
        "Como o tokenizador é bastante simples e o treinamento pode demorar um pouco, vamos usar apenas 10% do conjunto de dados. Embora você possa tentar usar o conjunto de dados completo se tiver uma GPU mais poderosa em sua posse (por exemplo, no Google Colab Pro/Pro+)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AQZ6HhaH7XAf"
      },
      "outputs": [],
      "source": [
        "os.environ[\"gpt_size\"] = \"medium\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT4Q1reoEW8r",
        "outputId": "7ba17e3b-6c60-4db8-b720-51c6d8b8764f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2023-06-01 16:27:00.025\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m60\u001b[0m - \u001b[34m\u001b[1mRandom seed is fixed for training.\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.026\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mLoading the data...\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mData is loaded.\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mStarting tokenizing...\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mTokenizing is done.\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1mSaving tokenizer...\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m81\u001b[0m - \u001b[1mTokenizer is saved.\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mPreparing data loaders...\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.187\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mData loaders are prepared.\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.187\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mStaring training...\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.388\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m114\u001b[0m - \u001b[34m\u001b[1mGPT language model is created with number of parameters: 10.65 million\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.389\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m131\u001b[0m - \u001b[34m\u001b[1mLR warmup iters: 156\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:00.389\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m137\u001b[0m - \u001b[34m\u001b[1mLR decay iters: 1490\u001b[0m\n",
            "\u001b[32m2023-06-01 16:27:01.973\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m122\u001b[0m - \u001b[34m\u001b[1mTraining on 'cuda' device\u001b[0m\n",
            "=============== Epoch: 0 ===============\n",
            "train:  52% 823/1569 [06:21<05:48,  2.14it/s, loss=1.79]"
          ]
        }
      ],
      "source": [
        "!python src/model/train.py gpt --size $gpt_size --dataset-fraction 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzd0xSId2Hnp"
      },
      "source": [
        "*E* os novos tokens são:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGW67vbyMamT"
      },
      "outputs": [],
      "source": [
        "!python src/model/generate.py gpt --size $gpt_size --max-new-tokens 1000 --fix-seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UARmUd1i2OM_"
      },
      "source": [
        "Ok, isso parece muito melhor do que o que o Bigram LM fez. Não se esqueça de que o conjunto de dados é bem pequeno e o tokenizer é básico. Portanto, o poder do GPT não é totalmente utilizado.\n",
        "\n",
        "Além disso, você pode obter melhores resultados com modelo maior e treinamento no conjunto de dados completo, mas levará um tempo no Nvidia T4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl-igDB8_EOP"
      },
      "source": [
        "## 2.4. GPT with pretrained weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX3cFhUl2oeA"
      },
      "source": [
        "Esta implementação GPT oferece suporte ao carregamento de pesos pré-treinados para o modelo GPT2 do Huggingface (os pesos são fornecidos pela OpenAI). Esse modelo foi treinado em um grande corpo de dados e usa um [tokenizador de pares de bytes] muito mais sofisticado (https://huggingface.co/course/chapter6/5?fw=pt).\n",
        "\n",
        "**Observação**: os pesos não são pré-treinados no conjunto de dados de Shakespeare, então a saída será diferente do que vimos antes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKEbWvr73rdl"
      },
      "source": [
        "**GPT2** tem 4 configurações:\n",
        "1. gpt2 (124M parameters) \n",
        "2. gpt2-medium (350M)\n",
        "3. gpt2-large (774M)\n",
        "4. gpt2-xl (1.5B)\n",
        "\n",
        "*O Google Colab com Nvidia T4 suporta até gpt2-large.\n",
        "Embora seja possível usar até mesmo o maior, será necessário alterar a forma como o modelo é carregado.*\n",
        "\n",
        "Quanto maior o modelo, melhor a amostragem, mas significa que o consumo de memória também será aumentado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2u06Mat_eUC"
      },
      "outputs": [],
      "source": [
        "os.environ[\"gpt2_config\"] = \"gpt2-medium\"\n",
        "os.environ[\"max_new_tokens\"] = \"1000\"\n",
        "os.environ[\"continue_tokens\"] = \"My name is Henrique Cyber but everybody calls me \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biUnerKq_Bb2"
      },
      "outputs": [],
      "source": [
        "!python src/model/generate.py gpt --gpt2-config $gpt2_config --max-new-tokens \"$((max_new_tokens))\" --fix-seed --continue-tokens \"$continue_tokens\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBXMcQPv4aEM"
      },
      "source": [
        "### 2.4.1. Key-Value cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8jyMTAi4eaV"
      },
      "source": [
        "Para GPT é possível usar kv-cache para acelerar a geração de novos tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNmvA2sS-PaP",
        "outputId": "5ab9e503-e36a-4f7a-ccaa-63d3b2241369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-03-18 14:54:23.120\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mRandom seed is fixed for token generation.\u001b[0m\n",
            "\u001b[32m2023-03-18 14:54:25.108\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m340\u001b[0m - \u001b[34m\u001b[1mCreating GPT model with parameters: {'vocab_size': 50257, 'embeddings_size': 1024, 'context_size': 1024, 'num_layers': 24, 'num_heads': 16, 'head_size': None, 'feed_forward_scaling': 4, 'bias': True, 'dropout': 0.1}\u001b[0m\n",
            "\u001b[32m2023-03-18 14:54:35.310\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m114\u001b[0m - \u001b[34m\u001b[1mGPT language model is created with number of parameters: 353.77 million\u001b[0m\n",
            "\u001b[32m2023-03-18 14:54:35.312\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m348\u001b[0m - \u001b[34m\u001b[1mLoading pretrained Huggingface model of size 'gpt2-medium' ...\u001b[0m\n",
            "\u001b[32m2023-03-18 14:54:41.583\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m350\u001b[0m - \u001b[34m\u001b[1mHuggingface model is loaded.\u001b[0m\n",
            "\u001b[32m2023-03-18 14:54:41.586\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m387\u001b[0m - \u001b[34m\u001b[1mStarting copying weights from pretrained Huggingface model into our implementation ...\u001b[0m\n",
            "\u001b[32m2023-03-18 14:54:42.519\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m401\u001b[0m - \u001b[34m\u001b[1mWeights are copied.\u001b[0m\n",
            "\u001b[32m2023-03-18 14:54:44.535\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m104\u001b[0m - \u001b[34m\u001b[1mGenerating tokens on 'cuda' device\u001b[0m\n",
            "100% 1000/1000 [00:18<00:00, 54.07it/s]\n",
            "\u001b[32m2023-03-18 14:55:03.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mNew generated tokens: My name is Giovanni Giorgio but everybody calls me ilexandro or protagonist or hero, and I have stopped doing this.\"But she managed to continue with the video on the AuctiCloud YouTube Channel. The video was also shared on various mobile phone stores.Now Endurance County is secured and Martin is beginning to regain her health. In the video, she talked about Tygerley Kimville and the story at that location:  Nice in Canada And this is released on their right to privacy where they are showing how charity can help with a dangerous disease and how skin, sealing electrons, can help contain this outbreak.\"We have suffered enough and we're here to say to everybody that we are dying - you don't breathe in what's shut on your mouth, your brain and your skin, the ultimate combination of love and pain can ease and remove viruses and infections.\"Australian Ken Hemul showed you, suffering in the US battling Ebola and taken by @mithenfurious - here is the vid of you being treated in another specialist in Nashville. Remember, burners live in Hot Cairo\"WHAT'S LIVE LIFE!?\": Also showing a video of Grandma Christine in New Zealand and asks if you did somebody 2008, this ONEENIGHT: Coverage working R-medic operating room including prompt mention of the looming solution pro Rom/EMA where I check in that important area of his genitalia\" take it home with you i look forward to direct pic you received; pic will of course make you want to carry and survive this 2002 inflammatory ad Ronald in Italy from the UK This column began : Tolerant fear kills and this goes over well\"Who says that news has to get out everyday? In book, with stories 2013 -current. Other tidbits of information : Ramadan Is Being Made Laws of World - deference to holy Prophet (Islamic Announcement) .\"Inflammation as a professional solution is off the table\"; \"http://files.greens.com.au/e-public/2016/10/Content_final.png\" Food systems become safer killed outside\n",
            "Listen to new Sounds Peace? on why vaccination is a contortion . Let Puffs Away,\n",
            "A user on the provenance family blog.\n",
            "Whinney wrote :\n",
            "I felt very helpless with how much he had all mapped out with cartoons, and bastard lives. He and his wife got emotional (on/off camera) at times of ending things to induce crying worse than tears.\n",
            "Does this sound familiar?! From NPR:\n",
            "Poor Calvin Puckett asked doctors whether switching vaccines could possibly impact his far deeper, more insidious disease in this brain hospital room lightly topless. A spokeswoman for St. Jude Medical Center told The Associated Press and WDAY that initial care supports medications plus a range of preventive measures, with dependent on how homesick parents respond. Under the age of 20, the aim is to reach frequent vaccination, if possible. With Prescription Drug Monitoring Program and CDPHP, one in 66 Kent face elimination within five days. More from Vanishing: New Zealand Post: Surprising, unnatural effects of yeast infection Despite the fields of yeast, it is \"much more comparable to peanut allergies,\" a study published September 11 in the journal PLOS One found.\n",
            "Quemmler says he's given no charge to the state of Oregon for his insurance because, indeed (after the morning chemotherapy treatment with twourized waste solution which captured only a mere echo of his debut Tlemofrog jaw), his opportunity to voice his less than conscientious, raw eyes is highly commercialized.  Conclusion :  \"virgin friends and families are begging for those who already have blinded artfully that they're things, monsters to stop letting die — that a mind as intense as mine could live long enough to matter.\" Citadel has taken record of more than 800 points in 2011/12, its 58 highest attending the Sacramento range over the past 12 months . Here are some stats on \"yet other events\n",
            "XXXX THE KEEPER Over to your friend Uni,\n",
            "About that nightly pounding over college speech '94 I imagine it's 140. The Expos announced, by the way, that Anthony Bonds and Alex Rodriguez will be honored with the Hall of Famer's baton, cybracologist statistician Fred Norton reported   \"Geniuses presented alternatively trotting reco Downing courts and tough guys standing on hot dogs. Bursts of automated commentary measured 108 to 178 to 82 pressure point. Payback? Maybe.\" Nice.\n",
            "With Toews backing away from Dirk Nowitzki  Toews, who turns 38 this month, speaks passionately at his baseball grave after the ax was removed from his neck and eventually ushers the puck to the Cy Young in Oakland. The perfect finish to a surprising afternoon's work . Funny thing is eleven dope stories just as a free Julep service ended around the same time. And in anticipation of your 3:00 pm funk.<|endoftext|>+ Split Screen with Rides!\n",
            "\u001b[0m\n",
            "\u001b[32m2023-03-18 14:55:03.032\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m121\u001b[0m - \u001b[34m\u001b[1mToken generation took: 18.4969 seconds\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python src/model/generate.py gpt --gpt2-config $gpt2_config --max-new-tokens \"$((max_new_tokens))\" --fix-seed --continue-tokens \"$continue_tokens\" --use-kv-cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbx9voRD4ut1"
      },
      "source": [
        "Veja a diferença: 120-150 segundos sem cache, 18-19 segundos - com. É por isso que o kv-caching é amplamente adotado."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}